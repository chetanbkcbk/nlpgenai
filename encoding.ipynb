{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame({\"text\":[\"people watch cricket\",\"cricket watch cricket\",\"people give comment\",\"cricket give comment\"],\"output\":[1,1,0,0]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch cricket</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cricket watch cricket</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people give comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cricket give comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  output\n",
       "0   people watch cricket       1\n",
       "1  cricket watch cricket       1\n",
       "2    people give comment       0\n",
       "3   cricket give comment       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now pip install scikit-learn in command prompt and then do the below\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW=CountVectorizer() #converts a collection of documents to matrix of token counts(ie,Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     people watch cricket\n",
       "1    cricket watch cricket\n",
       "2      people give comment\n",
       "3     cricket give comment\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"] #gives me documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 11 stored elements and shape (4, 5)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_matrix=BOW.fit_transform(data[\"text\"]) #it gives documnet matrix\n",
    "document_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 3, 'watch': 4, 'cricket': 1, 'give': 2, 'comment': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW.vocabulary_\n",
    "\n",
    "#this is my Bag of Words with UNIGRAM\n",
    "\n",
    "#the index is assignes based on alphabetical order ,as seen below\n",
    "\n",
    "# { 'word:index', etc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_matrix[0].toarray()#this gives the VECTOR of the first document/sentence ie)BOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_matrix[2].toarray()#this gives the vector of th third document/sentence ie)BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Home Work\n",
    "Implement OHE for the below data using ANY library(for example numpy, pandas, tf, pytorch etc...)\n",
    "after completeing this tag me and krish sir and send your solution in group chat\n",
    "in next class i will give you the quick overview of it'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N -GRAM   for that i have to crete an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "##CountVectorizer basically captures the information in sequence,\n",
    "# its not going to jump the word in btw,,,for jumping the words in btw, i have to write my own logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     people watch cricket\n",
       "1    cricket watch cricket\n",
       "2      people give comment\n",
       "3     cricket give comment\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data[\"text\"] ##it shows in which index document s are assigned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram=CountVectorizer(ngram_range=(2,2)) #bifram means 2 piar of words\n",
    "#that you want to extract bigrams (2-grams) from the text.\n",
    "\n",
    "#countvectorizer basically captures the information in sequence\n",
    "\n",
    "bigramvocab=bigram.fit_transform(data[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people watch': 4,\n",
       " 'watch cricket': 5,\n",
       " 'cricket watch': 1,\n",
       " 'people give': 3,\n",
       " 'give comment': 2,\n",
       " 'cricket give': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.vocabulary_ \n",
    "####this is my Bag Of Words with BIGRAMS\n",
    "\n",
    "\n",
    " #creates a pair of 2 words as below  ,\n",
    "\n",
    "#uniq vocab from the above data  looks like this internally\n",
    "# [('cricket give'),('cricket watch'),('give comment'),('people give'),('people watch'),('watch cricket')]\n",
    "\n",
    "#0,1,2,3 refers to the index based on the alphabetical order\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''explaination of above  2 cells\n",
    "\n",
    "bigram->: This variable is presumably an instance of CountVectorizer (or a similar vectorizer) that has been configured to extract bigrams.\n",
    "\n",
    "data[\"text\"]->: This part refers to a column named \"text\" in a DataFrame (likely a pandas DataFrame). The data variable is expected to be a DataFrame that contains text data. The \"text\" column contains the actual text documents from which you want to extract bigrams.'\n",
    "\n",
    "fit_transform(...): This method does two things:\n",
    "\n",
    "    Fit->: It learns the vocabulary of the bigrams from the provided text data. This means it identifies all the unique bigrams present in the text.\n",
    "\n",
    "    Transform->: It converts the text data into a matrix of token counts (in this case, counts of bigrams). Each row in the resulting matrix corresponds to a document, and each column corresponds to a unique bigram. The values in the matrix represent the count of each bigram in each document.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people watch cricket': 3,\n",
       " 'cricket watch cricket': 1,\n",
       " 'people give comment': 2,\n",
       " 'cricket give comment': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram=CountVectorizer(ngram_range=(3,3)) #bifram means 2 piar of words\n",
    "\n",
    "trigram.fit_transform(data[\"text\"])\n",
    "\n",
    "trigram.vocabulary_\n",
    "####this gives my Bag Of Words with TRIGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets see MIX Grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 8,\n",
       " 'watch': 13,\n",
       " 'cricket': 1,\n",
       " 'people watch': 11,\n",
       " 'watch cricket': 14,\n",
       " 'people watch cricket': 12,\n",
       " 'cricket watch': 4,\n",
       " 'cricket watch cricket': 5,\n",
       " 'give': 6,\n",
       " 'comment': 0,\n",
       " 'people give': 9,\n",
       " 'give comment': 7,\n",
       " 'people give comment': 10,\n",
       " 'cricket give': 2,\n",
       " 'cricket give comment': 3}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix=CountVectorizer(ngram_range=(1,3))\n",
    "#here both Unigram and bigrams and trigrams  will be shown in sequentialorder without jumping and the biggest advantage is here the  context will be obtained\n",
    "\n",
    "mix.fit_transform(data[\"text\"])\n",
    "\n",
    "mix.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aim of encoding and encoding was to convert data into vectors(numerical representation of data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF->invented by google\n",
    "\n",
    "#Term-Frequency means occurance of word in a document\n",
    "\n",
    "'''\n",
    "TF of a word =\n",
    "occurance of word in document/total words in that document\n",
    "'''\n",
    "\n",
    "#Inverse Document Frequency\n",
    "\n",
    "'''\n",
    "IDF of word =\n",
    "log(\n",
    "No of  total documents/\n",
    "no of documents containing this word\n",
    ")\n",
    "\n",
    "log helps normalise the values\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "to convert data into encoding involves converting into vectors\n",
    "\n",
    "step1)obtain vocabulary from the data\n",
    "\n",
    "\n",
    "\n",
    "so corpus->documents->inside each document we can get a specific number for each word using TF*IDF formula and at the end we can get to know the importance of each word based on that number '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TF-> gives Importance in a given sentence or document\n",
    "\n",
    "IDF->It shows the uniqueness of a word across all sentences\n",
    "\n",
    "log-> to compress or normalise the IDF value or reduce the skewness in IDF value\n",
    "\n",
    "TF*IDF-> we are reducing the weightage of more common word and giving imp to unique/rare words\n",
    "\n",
    "\n",
    "\n",
    "+ves->simple,context,unique words prioritsed\n",
    "\n",
    "-ves-> Out of Vocabulary still remains,also little spearsity remains\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so we are importing TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch cricket</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cricket watch cricket</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people give comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cricket give comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  output\n",
       "0   people watch cricket       1\n",
       "1  cricket watch cricket       1\n",
       "2    people give comment       0\n",
       "3   cricket give comment       0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     people watch cricket\n",
       "1    cricket watch cricket\n",
       "2      people give comment\n",
       "3     cricket give comment\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.49681612, 0.        , 0.61366674, 0.61366674],\n",
       "       [0.        , 0.8508161 , 0.        , 0.        , 0.52546357],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.57735027, 0.        ],\n",
       "       [0.61366674, 0.49681612, 0.61366674, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "#creating object of TF-IDF\n",
    "\n",
    "\n",
    "tfidf.fit_transform(data[\"text\"]).toarray()  \n",
    "#this gives me the TF_IDF vector for given data,basically number for each word which states the importance in the document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['comment' 'cricket' 'give' 'people' 'watch']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "print(\"Feature Names:\", feature_names) #will give me the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.51082562, 1.22314355, 1.51082562, 1.51082562, 1.51082562])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.idf_ #gives the idf value for each of the unique words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
